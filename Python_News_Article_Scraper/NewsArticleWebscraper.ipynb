{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "from bs4 import BeautifulSoup # To parse and extract data from HTML\n",
    "import requests # To send HTTP requests to the website\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import smtplib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American Ultrarunner Tara Dower Shatters Appalachian Trail Speed Record\n",
      "World's First Wooden Satellite Launched Into Space\n",
      "Make A Difference On GivingTuesday\n",
      "President Joe Biden Pardons Two Lucky Turkeys Ahead Of Thanksgiving\n",
      "Metal Detectorists Unearth Ancient Silver Coins Worth Millions\n",
      "Astronomers Capture First Close-Up Picture Of A Star Outside Our Galaxy\n",
      "This Ingenious Drone Recharges On The Go\n",
      "The Origins Of Some Beloved Thanksgiving Traditions\n",
      "Emperor Penguin Spotted On Western Australian Shores For The First Time\n",
      "Amateur Paleontologist Finds An Almost Complete Titanosaur Skeleton\n",
      "November's Beaver Moon Will Be This Year's Last Supermoon\n",
      "Remote Alaskan Town Will See Its Final Sunset Of 2024 On November 19th\n",
      "November Honors Native American Heritage And Contributions\n",
      "No Text found.\n"
     ]
    }
   ],
   "source": [
    "# connect to the website and get the article names\n",
    "\n",
    "# URL of the website we want to scrape\n",
    "URL = \"https://www.dogonews.com/\"\n",
    "\n",
    "# Find Your User-Agent: https://httpbin.org/get\n",
    "# Set headers to simulate a real browser request (some websites block non-browser traffic)\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Send an HTTP GET request to the URL, passing in the headers\n",
    "page = requests.get(URL, headers=headers)\n",
    "\n",
    "# Check if the page request was successful (status code 200)\n",
    "# If unsuccessful, raise an exception (not shown here, but you can add error handling)\n",
    "# We can check the status code of the response with `page.status_code`\n",
    "\n",
    "# Parse the page content using BeautifulSoup (HTML parsing)\n",
    "soup1 = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Use prettify() to get a pretty-formatted HTML string, then parse it again with BeautifulSoup\n",
    "# This step isn't strictly necessary but it can make the HTML more readable\n",
    "soup2 = BeautifulSoup(soup1.prettify(), 'html.parser')\n",
    "\n",
    "# Find all <h2> tags on the page, which are likely to contain the article titles\n",
    "# The article titles are often inside <a> tags nested within <h2> tags\n",
    "headings = soup2.find_all('h2')\n",
    "\n",
    "# initialize a list to store all the article titles of the day\n",
    "titles = []\n",
    "\n",
    "# Loop through each <h2> tag found on the page\n",
    "for heading in headings:\n",
    "    # Within each <h2> tag, try to find the <a> tag that contains the article title\n",
    "    parsedHtml = heading.find('a')\n",
    "\n",
    "    # If an <a> tag is found (i.e., the article title exists)\n",
    "    if parsedHtml:\n",
    "        # Extract the text content from the <a> tag and remove any extra whitespace (e.g., newlines)\n",
    "        title = parsedHtml.get_text().strip()\n",
    "        # add the article to the articles list\n",
    "        titles.append(title)\n",
    "        # Print the article title to the console\n",
    "        print(title)\n",
    "    else:\n",
    "        # If no <a> tag is found inside this <h2>, print a message (this usually shouldn't happen for valid articles)\n",
    "        print(\"No Text found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-05\n"
     ]
    }
   ],
   "source": [
    "# Create a Timestamp for your output to track when data was collected\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Get today's date\n",
    "today = datetime.date.today()\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "# Header for the CSV file (names of the columns)\n",
    "header = ['No.', 'Articles', 'Date']\n",
    "\n",
    "# Create a list of article numbers (e.g., 1, 2, 3, ...)\n",
    "No = [num for num in range(1,len(headings))]\n",
    "\n",
    "data = [No, titles, today]\n",
    "\n",
    "# Open the CSV file in write mode\n",
    "with open('DogoNewsArticlesList.csv', 'w', newline='', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # Write the header row to the CSV file\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # Write each article as a new row with the article number, title, and date\n",
    "    for num, title in zip(No, titles):\n",
    "        writer.writerow([num, title, today])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
