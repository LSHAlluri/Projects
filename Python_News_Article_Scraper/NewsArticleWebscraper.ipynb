{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.5",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": [
                "# import necessary libraries\n",
                "\n",
                "from bs4 import BeautifulSoup # To parse and extract data from HTML\n",
                "import requests # To send HTTP requests to the website\n",
                "import time\n",
                "import datetime\n",
                "\n",
                "import smtplib"
            ],
            "metadata": {
                "azdata_cell_guid": "94a6938c-ed2f-41cb-997c-6e14d1474e25",
                "language": "python"
            },
            "outputs": [],
            "execution_count": 2
        },
        {
            "cell_type": "code",
            "source": [
                "# connect to the website and get the article names\n",
                "\n",
                "# URL of the website we want to scrape\n",
                "URL = \"https://www.dogonews.com/\"\n",
                "\n",
                "# Find Your User-Agent: https://httpbin.org/get\n",
                "# Set headers to simulate a real browser request (some websites block non-browser traffic)\n",
                "headers = {\n",
                "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\"\n",
                "}\n",
                "\n",
                "# Send an HTTP GET request to the URL, passing in the headers\n",
                "page = requests.get(URL, headers=headers)\n",
                "\n",
                "# Check if the page request was successful (status code 200)\n",
                "# If unsuccessful, raise an exception (not shown here, but you can add error handling)\n",
                "# We can check the status code of the response with `page.status_code`\n",
                "\n",
                "# Parse the page content using BeautifulSoup (HTML parsing)\n",
                "soup1 = BeautifulSoup(page.content, \"html.parser\")\n",
                "\n",
                "# Use prettify() to get a pretty-formatted HTML string, then parse it again with BeautifulSoup\n",
                "# This step isn't strictly necessary but it can make the HTML more readable\n",
                "soup2 = BeautifulSoup(soup1.prettify(), 'html.parser')\n",
                "\n",
                "# Find all <h2> tags on the page, which are likely to contain the article titles\n",
                "# The article titles are often inside <a> tags nested within <h2> tags\n",
                "headings = soup2.find_all('h2')\n",
                "\n",
                "# initialize a list to store all the article titles of the day\n",
                "titles = []\n",
                "\n",
                "# Loop through each <h2> tag found on the page\n",
                "for heading in headings:\n",
                "    # Within each <h2> tag, try to find the <a> tag that contains the article title\n",
                "    parsedHtml = heading.find('a')\n",
                "\n",
                "    # If an <a> tag is found (i.e., the article title exists)\n",
                "    if parsedHtml:\n",
                "        # Extract the text content from the <a> tag and remove any extra whitespace (e.g., newlines)\n",
                "        title = parsedHtml.get_text().strip()\n",
                "        # add the article to the articles list\n",
                "        titles.append(title)\n",
                "        # Print the article title to the console\n",
                "        print(title)\n",
                "    else:\n",
                "        # If no <a> tag is found inside this <h2>, print a message (this usually shouldn't happen for valid articles)\n",
                "        print(\"No Text found.\")\n"
            ],
            "metadata": {
                "azdata_cell_guid": "a7c3bee6-746c-4d25-8edf-7e86192e2bf1",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Oops! Kentucky 2nd Grader Accidentally Orders Over 70,000 Lollipops!\nThriving Ecosystem Discovered Beneath Antarctic Ice\nWhy The European Space Agency Paid Volunteers Thousands To Lie Flat\nNo Text found.\nMia DaPonte Is America's Youngest Female Master Scuba Diver\nCardinal Robert Prevost Makes History As First American Pope\nThe Ice Bucket Challenge Returns With A New Purpose\nThe Papal Conclave Explained\nPhiladelphia Zoo Celebrates Birth Of Critically Endangered Tortoises\nGet Ready For Mother's Day!\nResearchers May Have Found A New Color!\nSharks May Not Be Silent After All!\nBaseballâ€™s New Power Weapon: The Torpedo Bat\nFree Comic Book Day Is On May 3rd!\nNo Text found.\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 5
        },
        {
            "cell_type": "code",
            "source": [
                "# Create a Timestamp for your output to track when data was collected\n",
                "\n",
                "import datetime\n",
                "\n",
                "# Get today's date\n",
                "today = datetime.date.today()\n",
                "print(today)"
            ],
            "metadata": {
                "azdata_cell_guid": "ec8183de-a3ec-4cab-8816-fcc9074c08b5",
                "language": "python"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "2025-05-17\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 4
        },
        {
            "cell_type": "code",
            "source": [
                "import csv\n",
                "\n",
                "\n",
                "# Header for the CSV file (names of the columns)\n",
                "header = ['No.', 'Articles', 'Date']\n",
                "\n",
                "# Create a list of article numbers (e.g., 1, 2, 3, ...)\n",
                "No = [num for num in range(1,len(headings))]\n",
                "\n",
                "data = [No, titles, today]\n",
                "\n",
                "# Open the CSV file in write mode\n",
                "with open('DogoNewsArticlesList.csv', 'w', newline='', encoding='UTF8') as f:\n",
                "    writer = csv.writer(f)\n",
                "\n",
                "    # Write the header row to the CSV file\n",
                "    writer.writerow(header)\n",
                "\n",
                "    # Write each article as a new row with the article number, title, and date\n",
                "    for num, title in zip(No, titles):\n",
                "        writer.writerow([num, title, today])"
            ],
            "metadata": {
                "azdata_cell_guid": "46acefd6-9ab3-44e2-870f-1fa7921068a3",
                "language": "python"
            },
            "outputs": [],
            "execution_count": 6
        }
    ]
}